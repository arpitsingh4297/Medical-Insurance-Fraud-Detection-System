# -*- coding: utf-8 -*-
"""Medical Insurance Fraud Detection.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mDK1Wz4yvCha36Pd-U_y1leCaIicDeFb
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_auc_score, roc_curve
from imblearn.over_sampling import SMOTE
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import IsolationForest

from xgboost import XGBClassifier
import category_encoders as ce
import shap

import warnings
warnings.filterwarnings('ignore')

from google.colab import drive
drive.mount('/content/drive')

# --- Define file paths for Google Colab ---
CLAIMS_PATH = '/content/drive/MyDrive/Medical Insurance Fraud Detection Datasets/claims.csv'
PATIENTS_PATH = '/content/drive/MyDrive/Medical Insurance Fraud Detection Datasets/patients.csv'
PAYMENTS_PATH = '/content/drive/MyDrive/Medical Insurance Fraud Detection Datasets/payments.csv'
PROVIDERS_PATH = '/content/drive/MyDrive/Medical Insurance Fraud Detection Datasets/providers.csv'
FRAUD_PATH = '/content/drive/MyDrive/Medical Insurance Fraud Detection Datasets/fraud.csv'

# --- Load your main datasets ---
try:
    claims = pd.read_csv(CLAIMS_PATH, low_memory=False)
    patients = pd.read_csv(PATIENTS_PATH, low_memory=False)
    payments = pd.read_csv(PAYMENTS_PATH, low_memory=False)
    providers = pd.read_csv(PROVIDERS_PATH, low_memory=False)
    print("âœ… Main datasets loaded successfully from Google Drive.")
except FileNotFoundError as e:
    print(f"âŒ Error loading main dataset: {e}. Make sure the files are in the correct Google Drive path and that you have mounted your Drive.")
    exit()

# Merge datasets
df = claims.merge(patients, on='patient_id', how='left')
df = df.merge(payments[['claim_id', 'payment_amount']], on='claim_id', how='left')
df = df.merge(providers, on='provider_id', how='left')
print(f"âœ… Final Merged Shape (Initial): {df.shape}")

# Load fraud labels
try:
    df_synthetic_fraud = pd.read_csv(FRAUD_PATH, low_memory=False)
    print("âœ… Synthetic fraud labels loaded successfully.")
except FileNotFoundError:
    print("âŒ Synthetic fraud labels file not found. Make sure the file is in the correct path.")
    exit()

# Consistent patient_id types
df['patient_id'] = df['patient_id'].astype(str)
df_synthetic_fraud['patient_id'] = df_synthetic_fraud['patient_id'].astype(str)

# Merge fraud labels
df = df.merge(df_synthetic_fraud[['patient_id', 'is_fraud']], on='patient_id', how='left')
df['is_fraud'].fillna(0, inplace=True)
print(f"âœ… Final Merged Shape with Fraud Labels: {df.shape}")
print("\nðŸ“Š Fraud class distribution:\n", df['is_fraud'].value_counts())

# Drop unnecessary columns
drop_cols = ['claim_id', 'patient_id', 'provider_id', 'status', 'claim_date', 'address', 'phone_x']
df.drop(columns=drop_cols, inplace=True, errors='ignore')

# Feature-target split
X = df.drop('is_fraud', axis=1)
y = df['is_fraud'].astype(int)

# Impute and encode
num_cols = X.select_dtypes(include=np.number).columns
cat_cols = X.select_dtypes(include='object').columns

if len(num_cols) > 0:
    imputer = SimpleImputer(strategy='mean')
    X[num_cols] = imputer.fit_transform(X[num_cols])

if len(cat_cols) > 0:
    encoder = ce.TargetEncoder(cols=cat_cols)
    X = encoder.fit_transform(X, y)
    print("\nâœ… Target encoding applied.")

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# SMOTE oversampling
smote = SMOTE(random_state=42)
X_train_res, y_train_res = smote.fit_resample(X_train, y_train)
print("\nðŸ“Š SMOTE class distribution:\n", pd.Series(y_train_res).value_counts())

# Models with faster XGBoost configuration
models = {
    "Logistic Regression": LogisticRegression(solver='liblinear', class_weight='balanced', random_state=42),
    "Random Forest": RandomForestClassifier(n_estimators=100, n_jobs=-1, class_weight='balanced', random_state=42),
    "XGBoost": XGBClassifier(
        use_label_encoder=False,
        eval_metric='logloss',
        random_state=42,
        scale_pos_weight=(y_train_res == 0).sum() / (y_train_res == 1).sum(),
        n_estimators=50,      # Reduced number of trees
        max_depth=2,          # Further reduced tree depth
        learning_rate=0.2,     # Increased learning rate
        subsample=0.7,          # Slightly more aggressive row subsampling
        colsample_bytree=0.7,   # Slightly more aggressive column subsampling
        n_jobs=-1,
        tree_method='hist'
    )
}

# Train & Evaluate
for name, model in models.items():
    print(f"\nðŸš€ Training {name}...")
    pipe = Pipeline([
        ('scaler', StandardScaler()),
        ('clf', model)
    ])
    pipe.fit(X_train_res, y_train_res)
    y_pred = pipe.predict(X_test)
    y_prob = pipe.predict_proba(X_test)[:, 1]

    print(f"\nðŸ“Š {name} Results:")
    print("âœ… Accuracy:", round(accuracy_score(y_test, y_pred), 4))
    print("ðŸ” Confusion Matrix:\n", confusion_matrix(y_test, y_pred))
    print("ðŸ“ˆ Classification Report:\n", classification_report(y_test, y_pred))
    print("ðŸŽ¯ AUC-ROC:", round(roc_auc_score(y_test, y_prob), 4))

    fpr, tpr, _ = roc_curve(y_test, y_prob)
    plt.figure(figsize=(8, 6))
    plt.plot(fpr, tpr, label=f'{name} (AUC = {roc_auc_score(y_test, y_prob):.4f})')
    plt.plot([0, 1], [0, 1], 'k--')
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title(f'ROC Curve - {name}')
    plt.legend()
    plt.show()

    if name == "XGBoost":  # Save the XGBoost model
        joblib.dump(pipe, "/content/drive/MyDrive/Medical Insurance Fraud Detection Datasets/xgboost_model.pkl")
        print("âœ… XGBoost model saved as xgboost_model.pkl")

# Save sample data for Streamlit
X_test.head(20).to_csv("/content/drive/MyDrive/Medical Insurance Fraud Detection Datasets/sample_patient_data.csv", index=False)
print("âœ… Saved sample_patient_data.csv")

# --- Conditional SHAP Explanation ---

if name in ["Random Forest", "XGBoost"]:
        try:
            explainer = shap.Explainer(pipe.named_steps['clf'])
            shap_values = explainer(X_test)
            shap.plots.bar(shap_values)
            plt.title(f"SHAP Summary - {name}")
            plt.show()
        except Exception as e:
            print(f"âš ï¸ SHAP explanation failed for {name}: {e}")

# Isolation Forest
print("\n--- Isolation Forest ---")
X_scaled = StandardScaler().fit_transform(X)
iso_forest = IsolationForest(contamination='auto', random_state=42)
df['anomaly_iforest'] = iso_forest.fit_predict(X_scaled)
df['anomaly_iforest'] = df['anomaly_iforest'].map({1: 0, -1: 1})
print("Anomaly counts:\n", df['anomaly_iforest'].value_counts())

top_anomalies = df[df['anomaly_iforest'] == 1].head(20)
print("\nTop Anomalies:\n", top_anomalies)

# ========== AUTOENCODER FOR UNSUPERVISED FRAUD DETECTION ==========

from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Dense
from tensorflow.keras.callbacks import EarlyStopping
from sklearn.metrics import classification_report, roc_auc_score
import tensorflow as tf

# Scale the features
scaler_ae = StandardScaler()
X_scaled = scaler_ae.fit_transform(X)

# Train-test split for autoencoder (unsupervised)
X_train_ae, X_test_ae = train_test_split(X_scaled, test_size=0.2, random_state=42)

input_dim = X_train_ae.shape[1]
encoding_dim = 64

# Autoencoder model
input_layer = Input(shape=(input_dim,))
encoded = Dense(128, activation='relu')(input_layer)
encoded = Dense(encoding_dim, activation='relu')(encoded)
decoded = Dense(128, activation='relu')(encoded)
decoded = Dense(input_dim, activation='linear')(decoded)

autoencoder = Model(inputs=input_layer, outputs=decoded)
autoencoder.compile(optimizer='adam', loss='mse')

# Train autoencoder
autoencoder.fit(
    X_train_ae, X_train_ae,
    epochs=50,
    batch_size=256,
    shuffle=True,
    validation_split=0.2,
    callbacks=[EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)],
    verbose=1
)

# Predictions and MSE
reconstructions = autoencoder.predict(X_test_ae)
mse = np.mean(np.square(X_test_ae - reconstructions), axis=1)
threshold = np.percentile(mse, 95)
y_pred_ae = (mse > threshold).astype(int)

# Use actual fraud labels for this split
_, y_test_ae = train_test_split(y, test_size=0.2, random_state=42)

print("\nðŸ“Š Autoencoder Results")
print("ROC AUC:", roc_auc_score(y_test_ae, mse))
print("Classification Report:\n", classification_report(y_test_ae, y_pred_ae))

# Save reconstruction results
results = pd.DataFrame({
    "reconstruction_error": mse,
    "true_label": y_test_ae.values,
    "predicted_label": y_pred_ae
})
results.to_csv("unsupervised_fraud_detection_results.csv", index=False)

# Plot first 50 predictions
sample_idx = np.arange(50)
plt.figure(figsize=(14, 6))
bar_width = 0.4
plt.bar(sample_idx, y_test_ae[:50], width=bar_width, label='Actual')
plt.bar(sample_idx + bar_width, y_pred_ae[:50], width=bar_width, label='Predicted', color='orange')
plt.title("Actual vs Predicted Labels (Autoencoder, First 50 Samples)")
plt.xlabel("Sample Index")
plt.ylabel("Label")
plt.legend()
plt.tight_layout()
plt.show()

